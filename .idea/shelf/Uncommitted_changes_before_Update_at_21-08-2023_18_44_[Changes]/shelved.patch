Index: Practice_1.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pyspark.sql import SparkSession\r\nfrom pyspark.sql.types import *\r\nfrom pyspark.sql.functions import *\r\n\r\n\r\nsc =SparkSession.builder.getOrCreate()\r\nrdd1=sc.sparkContext.parallelize([1,2,3])\r\nprint(rdd1.collect())  #parallelize method to collect the data through list.\r\n\r\nprint(\"number of partitions are: \"+str(rdd1.getNumPartitions()))  # Number of partitions\r\nrdd2=rdd1.repartition(2)  # repartition of cores\r\n\r\nprint(\"After repartition, number of partitions are: \",str(rdd2.getNumPartitions()))\r\nprint(rdd1.collect())\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Practice_1.py b/Practice_1.py
--- a/Practice_1.py	(revision 6133de9bb25c70cd90450d665ea971c5f172831c)
+++ b/Practice_1.py	(date 1692348826199)
@@ -1,14 +1,151 @@
+import pyspark
 from pyspark.sql import SparkSession
 from pyspark.sql.types import *
 from pyspark.sql.functions import *
 
 
 sc =SparkSession.builder.getOrCreate()
-rdd1=sc.sparkContext.parallelize([1,2,3])
-print(rdd1.collect())  #parallelize method to collect the data through list.
-
-print("number of partitions are: "+str(rdd1.getNumPartitions()))  # Number of partitions
-rdd2=rdd1.repartition(2)  # repartition of cores
-
-print("After repartition, number of partitions are: ",str(rdd2.getNumPartitions()))
+data=[1,2,3,4,5,6,7,8,9,10]
+#Parallelize method to read list
+rdd1=sc.sparkContext.parallelize(data,5)
 print(rdd1.collect())
+
+#textfile method to get the file from user
+rdd2=sc.sparkContext.textFile("C:/Users/VinayBoddu/Documents/sparkfolder/sparkfile.txt")
+rdd3=rdd2.count()  #count is used to count number of lines in text file
+print("Number of lines in text file is", rdd3)
+
+#wholetextfile method to read multiple files in given file path
+rdd4=sc.sparkContext.wholeTextFiles("C:/Users/VinayBoddu/Documents/sparkfolder/sparkfile2.txt")
+#To print, number of files available
+count=rdd4.count()
+print("Number of files are ",count)
+#Below command used to print filepaths and filecontents:
+for filepath, filecontent in rdd4.collect():
+    print("file path: ",filepath)
+    print("file content: ",filecontent)
+    print("--------")
+
+
+#Empty RDD
+rdd5=sc.sparkContext.parallelize([]) #using Parallelize
+rdd2=sc.sparkContext.emptyRDD() #using Empty RDD
+print(rdd5.count())
+print(rdd2.collect())
+
+#Empty RDD with partitions
+rdd5=sc.sparkContext.parallelize([],10)
+print(rdd5.count(), rdd5.getNumPartitions())
+
+#Repartioning
+rdd5=rdd1.repartition(2)
+print(rdd5.collect())
+print("After reparitioning, number of paritions are ",rdd5.getNumPartitions())
+
+#Reading CSV file:
+csv_file_path="C:/Users/VinayBoddu/Desktop/roll.csv"
+df1=sc.read.csv(csv_file_path,header=True, inferSchema=True)
+print(df1.show())   #showing results of csv file
+print(df1.printSchema())  #printing the schema of the file
+
+#Creating Broadcast variable
+data=[1,2,3,4,5]
+rdd5=sc.sparkContext.broadcast(data)
+def process_data(value):
+    # Access the broadcast variable's value
+    broadcasted_data = rdd5.value
+    result = value * broadcasted_data[4]  # Perform some operation
+    return result
+rdd = sc.sparkContext.parallelize([10, 20, 30, 40, 50])
+transformed_rdd = rdd.map(process_data)
+print(transformed_rdd.collect())
+
+#Accumulator example
+accum=sc.sparkContext.accumulator(0) #Initialized accumulator with 0 value
+
+#Updating accumulator variable (This can be done within driver program)
+rdd2=sc.sparkContext.parallelize([1,2,3,4,5])
+def count_update(value):
+    global accum
+    accum=accum+1
+    return value
+rdd3=rdd2.map(count_update)
+print("Accumulated variable count is ",accum.value)  #We can get accumulated variable count by using value attribute
+
+#RDD to Dataframe
+rdd3=sc.sparkContext.parallelize([(1,'vinay'),(2,'viru'),(3,'virat'),(4,'dhoni')])
+df=rdd3.toDF(["id","name"])
+df.show() #show methods is used to show the records present in dataframe
+
+#Using CreateDataframe
+rdd3=sc.sparkContext.parallelize([Row(id=1,name="vinay"),Row(id=2,name="Virender")])
+schema=StructType([StructField("id",IntegerType(),True),StructField("name",StringType(),False)])
+df=sc.createDataFrame(rdd3,schema=schema)
+print(df.show())
+
+#Creating Dataframe using Python Dictionary:
+# data={"name":['vinay','viru','rohit','virat'],"scores":[20,30,40,50]}
+# schema=StructType([StructField("Name",StringType(),True),StructField("age",IntegerType(),True)])
+# df=sc.createDataFrame(data,schema=schema)
+# print(df.show())
+
+#Selecting required columns in dataframe
+rdd3=sc.sparkContext.parallelize([(1,"vinay",30),(2,"virender",40),(3,"Charan",50)])
+# schema=StructType([StructField("id",IntegerType(),True),StructField("name",StringType(),True),StructField("marks",IntegerType(),True)])
+schema=['id','name','marks']
+df1=sc.createDataFrame(data=rdd3,schema=schema)
+df2=df1.select("id","name")
+df2.show()
+
+#withColumn(), to add or update columns
+data=sc.sparkContext.parallelize([(1,"vinay",20,"maths"),(2,"virender",30,"science")])
+# data2=sc.sparkContext.parallelize(["Swimming"])
+column=["id","name","marks","subject"]
+df1=sc.createDataFrame(data=data,schema=column)
+df2=df1.withColumn("Hobbies",lit("Swimming")) #New column added
+df3=df2.withColumn("marks",df2["marks"]+100) #Updated existing column
+df3.show()
+
+#Renaming Nested column
+data=sc.sparkContext.parallelize([("vinay",{"sno":1,"subject":"Maths","hobby":"Swimming"}),("Virender",{"sno":2,"subject":"Social","hobby":"Badmintion"})])
+column=StructType([
+    StructField("name",StringType()),
+    StructField("info",StructType([
+        StructField("sno",IntegerType()),
+        StructField("subject",StringType()),
+        StructField("hobby",StringType())
+    ]))
+])
+df=sc.createDataFrame(data=data,schema=column)
+df2=(df.withColumnRenamed("info","details")
+     .withColumnRenamed("name","Newname"))
+df2.show(truncate=False)
+
+#Dropping columns
+df3=df2.drop("Newname")
+df3.show()
+
+#Where Filter in Pyspark
+df2=df.where(df.name=="vinay")
+df2.show()
+
+#When Otherwise in Pyspark
+data=[("vinay",30),("virender",40),("Charan",50),("Paras",60),("vinay",70),("vinay",80)]
+column=["Name","Marks"]
+df1=sc.createDataFrame(data=data,schema=column)
+df2=df1.withColumn("Grade",when(df1.Marks>40,"Pass").otherwise("Fail"))
+df2.show()
+
+#Collect functinon(this is expensive)
+df3=df1.collect()
+for row in df3:
+    print(row)
+
+#Distinct values in pyspark
+df3=df1.select("name").distinct()
+df3.show()
+print("Count of distinct values is",df3.distinct().count())
+
+#Pivot Example
+df3=df2.groupBy("Name").pivot("Grade").sum("Marks")
+df3.show()
Index: resources/data.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources/data.txt b/resources/data.txt
new file mode 100644
--- /dev/null	(date 1692352349333)
+++ b/resources/data.txt	(date 1692352349333)
@@ -0,0 +1,5 @@
+DEBUG, 2017-03-23T10:02:27+00:00, ghtorrent-40 -- ghtorrent.rb: Repo EFForg/https-everywhere exists
+DEBUG, 2017-03-24T12:06:23+00:00, ghtorrent-49 -- ghtorrent.rb: Repo Shikanime/print exists
+INFO, 2017-03-23T13:00:55+00:00, ghtorrent-42 -- api_client.rb: Successful request. URL: https://api.github.com/repos/CanonicalLtd/maas-docs/issues/365/events?per_page=100, Remaining: 4943, Total: 88 ms
+WARN, 2017-03-23T20:04:28+00:00, ghtorrent-13 -- api_client.rb: Failed request. URL: https://api.github.com/repos/greatfakeman/Tabchi/commits?sha=Tabchi&per_page=100, Status code: 404, Status: Not Found, Access: ac6168f8776, IP: 0.0.0.0, Remaining: 3031
+DEBUG, 2017-03-23T09:06:09+00:00, ghtorrent-2 -- ghtorrent.rb: Transaction committed (11 ms)
Index: test/Assignment_3_test/Assignment_3_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/Assignment_3_test/Assignment_3_test.py b/test/Assignment_3_test/Assignment_3_test.py
new file mode 100644
--- /dev/null	(date 1692351673636)
+++ b/test/Assignment_3_test/Assignment_3_test.py	(date 1692351673636)
@@ -0,0 +1,10 @@
+import unittest
+
+
+class MyTestCase(unittest.TestCase):
+    def test_something(self):
+        self.assertEqual(True, False)  # add assertion here
+
+
+if __name__ == '__main__':
+    unittest.main()
Index: resources/Product_transaction.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources/Product_transaction.csv b/resources/Product_transaction.csv
new file mode 100644
--- /dev/null	(date 1692351343515)
+++ b/resources/Product_transaction.csv	(date 1692351343515)
@@ -0,0 +1,4 @@
+SourceId,TransactionNumber,Language,ModelNumber,StartTime,ProductNumber
+150711,123456,EN,456789,2021-12-27T08:20:29.842+0000,1
+150439,234567,US,345678,2021-12-27T08:21:14.645+0000,2
+150647,345678,ES,234567,2021-12-27T08:22:42.445+0000,3
Index: resources/product.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources/product.csv b/resources/product.csv
new file mode 100644
--- /dev/null	(date 1692351361428)
+++ b/resources/product.csv	(date 1692351361428)
@@ -0,0 +1,4 @@
+Product Name,Issue Date,Price,Brand,Country,Product Number
+Washung Machine,1.65E+12,20000, Samsung,India,1
+Refridgerator,1.65E+12,35000,   LG,null,2
+Air Cooler,1.65E+12,45000,   Voltas,null,3
Index: .idea/inspectionProfiles/profiles_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
--- /dev/null	(date 1691148363108)
+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1691148363108)
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
Index: resources/transactions.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources/transactions.csv b/resources/transactions.csv
new file mode 100644
--- /dev/null	(date 1692341920011)
+++ b/resources/transactions.csv	(date 1692341920011)
@@ -0,0 +1,11 @@
+transaction_id,product_id,userid,price,product_description
+3300101,1000001,101,700,mouse
+3300102,1000002,102,900,keyboard
+3300103,1000003,103,34000,tv
+3300104,1000004,101,35000,fridge
+3300105,1000005,105,55000,sofa
+3300106,1000006,106,100,bed
+3300107,1000007,105,66000,laptop
+3300108,1000008,108,20000,phone
+3300109,1000009,101,500,speaker
+3300110,1000010,102,1000,chair
Index: resources/users.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources/users.csv b/resources/users.csv
new file mode 100644
--- /dev/null	(date 1692341915154)
+++ b/resources/users.csv	(date 1692341915154)
@@ -0,0 +1,11 @@
+user_id,emailid,nativelanguage,location 
+101,abc.123@gmail.com,hindi,mumbai
+102,jhon@gmail.com,english,usa
+103,madan.44@gmail.com,marathi,nagpur
+104,local.88@outlook.com,tamil,chennai
+105,sahil.55@gmail.com,english,usa
+106,adi@gmail.com,hindi,nagpur
+107,jason@gmail.com,marathi,mumbai
+108,sohan@gmail.com,kannad,usa
+109,case@outlook.com,tamil,mumbai
+110,fury@gmail.com,hindi,nagpur
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
new file mode 100644
--- /dev/null	(date 1692623269273)
+++ b/.idea/workspace.xml	(date 1692623269273)
@@ -0,0 +1,195 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="AutoImportSettings">
+    <option name="autoReloadType" value="SELECTIVE" />
+  </component>
+  <component name="ChangeListManager">
+    <list default="true" id="4d7c5bd8-995d-46a0-9859-549591f5727a" name="Changes" comment="Driver file for Assignment.">
+      <change afterPath="$PROJECT_DIR$/.idea/inspectionProfiles/profiles_settings.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/Assignment_1/Driver.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/Assignment_1/util.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/PysparkRepo" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources.data.log" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources.data.txt" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources/Product_transaction.csv" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources/data.txt" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources/product.csv" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources/transactions.csv" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/resources/users.csv" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/src/Assignment_2/util.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/src/Assignment_3/driver.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/src/Assignment_3/util.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/test/Assignment_3_test/Assignment_3_test.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Practice_1.py" beforeDir="false" afterPath="$PROJECT_DIR$/Practice_1.py" afterDir="false" />
+    </list>
+    <option name="SHOW_DIALOG" value="false" />
+    <option name="HIGHLIGHT_CONFLICTS" value="true" />
+    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
+    <option name="LAST_RESOLUTION" value="IGNORE" />
+  </component>
+  <component name="FileTemplateManagerImpl">
+    <option name="RECENT_TEMPLATES">
+      <list>
+        <option value="Python Unit Test" />
+        <option value="Python Script" />
+      </list>
+    </option>
+  </component>
+  <component name="Git.Settings">
+    <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
+  </component>
+  <component name="GitHubPullRequestSearchHistory">{
+  &quot;lastFilter&quot;: {
+    &quot;state&quot;: &quot;OPEN&quot;,
+    &quot;assignee&quot;: &quot;VinayBoddu12&quot;
+  }
+}</component>
+  <component name="GithubPullRequestsUISettings">
+    <option name="selectedUrlAndAccountId">
+      <UrlAndAccount>
+        <option name="accountId" value="3fae47e7-d95f-4801-8d61-0f827a13e54a" />
+        <option name="url" value="https://github.com/VinayBoddu12/PysparkRepo.git" />
+      </UrlAndAccount>
+    </option>
+  </component>
+  <component name="MarkdownSettingsMigration">
+    <option name="stateVersion" value="1" />
+  </component>
+  <component name="ProjectColorInfo">{
+  &quot;associatedIndex&quot;: 7
+}</component>
+  <component name="ProjectId" id="2TShXmw7iGy6V9ud5f7wa1xmPlW" />
+  <component name="ProjectLevelVcsManager" settingsEditedManually="true" />
+  <component name="ProjectViewState">
+    <option name="hideEmptyMiddlePackages" value="true" />
+    <option name="showLibraryContents" value="true" />
+  </component>
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;git-widget-placeholder&quot;: &quot;main&quot;,
+    &quot;last_opened_file_path&quot;: &quot;C:/Users/VinayBoddu/Documents/PysparkRepo&quot;,
+    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;
+  }
+}</component>
+  <component name="RecentsManager">
+    <key name="CopyFile.RECENT_KEYS">
+      <recent name="C:\Users\VinayBoddu\Documents\PysparkRepo" />
+    </key>
+    <key name="MoveFile.RECENT_KEYS">
+      <recent name="C:\Users\VinayBoddu\Documents\PysparkRepo" />
+    </key>
+  </component>
+  <component name="RunManager">
+    <configuration name="main" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
+      <module name="PysparkRepo" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+  </component>
+  <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
+  <component name="TaskManager">
+    <task active="true" id="Default" summary="Default task">
+      <changelist id="4d7c5bd8-995d-46a0-9859-549591f5727a" name="Changes" comment="" />
+      <created>1691042430842</created>
+      <option name="number" value="Default" />
+      <option name="presentableId" value="Default" />
+      <updated>1691042430842</updated>
+    </task>
+    <task id="LOCAL-00001" summary="Practice file for Pyspark.">
+      <option name="closed" value="true" />
+      <created>1691071215471</created>
+      <option name="number" value="00001" />
+      <option name="presentableId" value="LOCAL-00001" />
+      <option name="project" value="LOCAL" />
+      <updated>1691071215471</updated>
+    </task>
+    <task id="LOCAL-00002" summary="Practice file for Pyspark.">
+      <option name="closed" value="true" />
+      <created>1691071329205</created>
+      <option name="number" value="00002" />
+      <option name="presentableId" value="LOCAL-00002" />
+      <option name="project" value="LOCAL" />
+      <updated>1691071329205</updated>
+    </task>
+    <task id="LOCAL-00003" summary="Practice file for Pyspark.">
+      <option name="closed" value="true" />
+      <created>1691071403889</created>
+      <option name="number" value="00003" />
+      <option name="presentableId" value="LOCAL-00003" />
+      <option name="project" value="LOCAL" />
+      <updated>1691071403889</updated>
+    </task>
+    <task id="LOCAL-00004" summary="Practice file for Pyspark.">
+      <option name="closed" value="true" />
+      <created>1691071797199</created>
+      <option name="number" value="00004" />
+      <option name="presentableId" value="LOCAL-00004" />
+      <option name="project" value="LOCAL" />
+      <updated>1691071797199</updated>
+    </task>
+    <task id="LOCAL-00005" summary="Practice file for Pyspark.">
+      <option name="closed" value="true" />
+      <created>1691072291774</created>
+      <option name="number" value="00005" />
+      <option name="presentableId" value="LOCAL-00005" />
+      <option name="project" value="LOCAL" />
+      <updated>1691072291774</updated>
+    </task>
+    <task id="LOCAL-00006" summary="Test file for Assignments.">
+      <option name="closed" value="true" />
+      <created>1692622353167</created>
+      <option name="number" value="00006" />
+      <option name="presentableId" value="LOCAL-00006" />
+      <option name="project" value="LOCAL" />
+      <updated>1692622353167</updated>
+    </task>
+    <task id="LOCAL-00007" summary="Driver file for Assignment.">
+      <option name="closed" value="true" />
+      <created>1692622959737</created>
+      <option name="number" value="00007" />
+      <option name="presentableId" value="LOCAL-00007" />
+      <option name="project" value="LOCAL" />
+      <updated>1692622959737</updated>
+    </task>
+    <option name="localTasksCounter" value="8" />
+    <servers />
+  </component>
+  <component name="Vcs.Log.Tabs.Properties">
+    <option name="TAB_STATES">
+      <map>
+        <entry key="MAIN">
+          <value>
+            <State />
+          </value>
+        </entry>
+      </map>
+    </option>
+  </component>
+  <component name="VcsManagerConfiguration">
+    <MESSAGE value="Practice file for Pyspark." />
+    <MESSAGE value="Test file for Assignments." />
+    <MESSAGE value="Driver file for Assignment." />
+    <option name="LAST_COMMIT_MESSAGE" value="Driver file for Assignment." />
+  </component>
+</project>
\ No newline at end of file
Index: resources.data.log
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources.data.log b/resources.data.log
new file mode 100644
--- /dev/null	(date 1692355639574)
+++ b/resources.data.log	(date 1692355639574)
@@ -0,0 +1,12 @@
+2023-08-18 16:17:12,401 [INFO] spark session created
+2023-08-18 16:17:12,827 [INFO] creating data frame
+2023-08-18 16:17:14,557 [INFO] number of lines:12
+2023-08-18 16:17:16,882 [INFO] reading an text file
+2023-08-18 16:17:17,062 [INFO] Creating a dataframe
+2023-08-18 16:17:18,255 [INFO] Number of warning messages:0
+2023-08-18 16:17:18,405 [INFO] Repositories processed only api clients:0
+2023-08-18 16:17:18,451 [INFO] most_http_requests
+2023-08-18 16:17:18,826 [INFO] Most failed requests
+2023-08-18 16:17:18,979 [INFO] most active hours
+2023-08-18 16:17:19,432 [INFO] most_active_repositories
+2023-08-18 16:17:19,574 [INFO] Closing down clientserver connection
Index: resources.data.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/resources.data.txt b/resources.data.txt
new file mode 100644
--- /dev/null	(date 1692355495712)
+++ b/resources.data.txt	(date 1692355495712)
@@ -0,0 +1,12 @@
+2023-08-18 16:14:48,099 [INFO] spark session created
+2023-08-18 16:14:48,524 [INFO] creating data frame
+2023-08-18 16:14:50,352 [INFO] number of lines:2
+2023-08-18 16:14:52,881 [INFO] reading an text file
+2023-08-18 16:14:53,053 [INFO] Creating a dataframe
+2023-08-18 16:14:54,244 [INFO] Number of warning messages:0
+2023-08-18 16:14:54,433 [INFO] Repositories processed only api clients:0
+2023-08-18 16:14:54,491 [INFO] most_http_requests
+2023-08-18 16:14:54,911 [INFO] Most failed requests
+2023-08-18 16:14:55,110 [INFO] most active hours
+2023-08-18 16:14:55,580 [INFO] most_active_repositories
+2023-08-18 16:14:55,712 [INFO] Closing down clientserver connection
Index: Assignment_1/util.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Assignment_1/util.py b/Assignment_1/util.py
new file mode 100644
--- /dev/null	(date 1692344127435)
+++ b/Assignment_1/util.py	(date 1692344127435)
@@ -0,0 +1,70 @@
+from pyspark.sql import SparkSession
+from pyspark.sql.functions import *
+
+
+sc = SparkSession.builder.getOrCreate()
+
+data = [("Washing Machine", 1648770933000, 20000, "Samsung", "India", "0001"),
+        ("Refrigerator", 1648770999000, 35000, "  LG", None, "0002"),
+        ("Air Cooler", 1648770948000, 45000, "  Voltas", None, "0003")]
+
+column = ["ProductName", "IssueDate", "Price", "Brand", "Country", "ProductNumber"]
+df = sc.createDataFrame(data=data,schema=column)
+
+# Convert "Issue Date" to timestamp format
+df2= df.withColumn("IssueDate", from_unixtime(df["IssueDate"]/1000, "yyyy-MM-dd'T'HH:mm:ss.SSSZ"))
+# Convert timestamp to date type
+df3= df2.withColumn("IssueDate", date_format(df2["IssueDate"], "yyyy-MM-dd"))
+# Remove extra spaces
+df4 = df3.withColumn("Brand", trim(df3["Brand"]))
+# null values to empty string
+df_final = df4.withColumn("Country", when(df4["Country"].isNull(), "").otherwise(df4["Country"]))
+# Show the final DataFrame
+df_final.show(truncate=False)
+
+
+
+# Sample data
+data = [
+    (150711, 123456, "EN", 456789, "2021-12-27T08:20:29.842+0000", "0001"),
+    (150439, 234567, "UK", 345678, "2021-12-27T08:21:14.645+0000", "0002"),
+    (150647, 345678, "ES", 234567, "2021-12-27T08:22:42.445+0000", "0003")
+]
+
+columns = ["SourceId", "TransactionNumber", "Language", "ModelNumber", "StartTime", "ProductNumber"]
+
+# Create a DataFrame
+df = sc.createDataFrame(data, columns)
+
+# Change camel case columns to snake case
+snake_case_columns = df.select([c.lower() for c in df.columns])
+snake_case_columns.show()
+
+# Add a column for start_time_ms
+df_with_ms = snake_case_columns.withColumn(
+    "start_time_ms",
+    expr("UNIX_TIMESTAMP(df_snake_case.start_time, 'yyyy-MM-dd'T'HH:mm:ss.SSSZ') * 1000"))
+# Show the final DataFrame
+df_with_ms.show(truncate=False)
+
+df4=sc.sql("""SELECT
+    df_with_ms.SourceId,
+    df_with_ms.TransactionNumber,
+    df_with_ms.Language,
+    df_with_ms.ModelNumber,
+    df_with_ms.StartTime,
+    df_with_ms.ProductNumber,
+    df_final.Price,
+    df_final.Brand,
+    'EN' AS Country,
+    df_final.ProductName
+FROM
+    df_final t1
+JOIN
+    df_with_ms t2
+ON
+    t1.ProductNumber = t2.ProductNumber""")
+df4.show()
+
+
+
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
new file mode 100644
--- /dev/null	(date 1692619597951)
+++ b/.idea/vcs.xml	(date 1692619597951)
@@ -0,0 +1,7 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+    <mapping directory="$PROJECT_DIR$/PysparkRepo" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100644
--- /dev/null	(date 1691148363043)
+++ b/.idea/misc.xml	(date 1691148363043)
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.9 (PysparkRepo)" project-jdk-type="Python SDK" />
+</project>
\ No newline at end of file
diff --git a/src/Assignment_3/driver.py b/src/Assignment_3/driver.py
new file mode 100644
diff --git a/src/Assignment_3/util.py b/src/Assignment_3/util.py
new file mode 100644
diff --git a/Assignment_1/Driver.py b/Assignment_1/Driver.py
new file mode 100644
